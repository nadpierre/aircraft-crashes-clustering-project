{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft Crashes Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores different clustering algorithms and compares the different results. The data has already been cleaned in the [EDA phase](data_analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data/crashes_cleaned_data2.pkl', 'rb') as handle:\n",
    "\tdf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registration number and manufacture serial number are unique identifiers of an aircraft\n",
    "df = df.drop(['registration', 'msn'], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "'date',\n",
    "'type',\n",
    "'registration',\n",
    "'msn',\n",
    "'yom',\n",
    "'engine',\n",
    "'engine_model',\n",
    "'aircraft_flying_hours',\n",
    "'aircraft_flight_cycles',\n",
    "'category',\n",
    "'flight_phase',\n",
    "'flight_type',\n",
    "'departure',\n",
    "'destination',\n",
    "'site',\n",
    "'location',\n",
    "'country',\n",
    "'region',\n",
    "'latitude',\n",
    "'longitude',\n",
    "'aircraft_damage',\n",
    "'occupants',\n",
    "'fatalities',\n",
    "'other_fatalities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split date in three columns: year, month, day\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df = df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce cardinality of categorical values\n",
    "columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in columns:\n",
    "  values = list(df[column].value_counts().head(4).index)\n",
    "  df[column] = np.where(~df[column].isin(values), 'Other', df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ordinal Encoding for aircraft_damage \n",
    "df['aircraft_damage'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder(categories=df['aircraft_damage'].cat.categories)\n",
    "encoded_col = ordinal_encoder.fit_transform(df['aircraft_damage'])\n",
    "#df['aircraft_damage'] = encoded_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use One Hot Encoding for other columns\n",
    "string_columns = []\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_cols = onehot_encoder.fit_transform(df[string_columns])\n",
    "new_df = pd.DataFrame(encoded_cols, columns=onehot_encoder.get_feature_names_out(string_columns))\n",
    "df = df.drop(columns=string_columns, axis=1).join(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert all columns are numeric\n",
    "assert len(df.columns) == len(df.select_dtypes([np.number]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation between variables\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr_matrix, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustermap\n",
    "sns.clustermap(corr_matrix, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "df.to_csv('data/crashes_preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality to 10\n",
    "pca = PCA(n_components=10, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "X.to_csv('data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "scaled_X = pd.DataFrame(scaled_X, columns=scaler.get_feature_names_out())\n",
    "scaled_X.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple models\n",
    "ssd = []\n",
    "for k in range(2, 31):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_X)\n",
    "    labels = kmeans.labels_\n",
    "    ssd.append({\n",
    "        'k': k,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'silhouette': silhouette_score(scaled_X, labels, metric='euclidean')\n",
    "\t})\n",
    "\n",
    "models = pd.DataFrame(ssd)\n",
    "models['difference'] = models['inertia'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method plot\n",
    "plt.plot(models['k'], models['inertia'], marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (SSD)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette method plot\n",
    "plt.plot(models['k'], models['silhouette'], marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best k value\n",
    "kmeans = KMeans(n_clusters=13, random_state=42)\n",
    "kmeans.fit(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('data/crashes_clean_data.csv')\n",
    "original_df.insert(1, 'Cluster', kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dendogram for optimal cluster determination\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.hlines(y=190,xmin=0,xmax=2000,lw=3,linestyles='--')\n",
    "plt.text(x=900,y=220,s='Horizontal line crossing 5 vertical lines',fontsize=20)\n",
    "#plt.grid(True)\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "ac = AgglomerativeClustering(n_clusters= 5, metric='euclidean', linkage='ward')\n",
    "ac.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df['Cluster'] = ac.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "min_samples = 2 * len(scaled_X.columns)\n",
    "\n",
    "for eps in np.linspace(0.001, 3, 50):\n",
    "\tdbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\tdbscan.fit(scaled_X)\n",
    "\n",
    "\tunique, counts = np.unique(dbscan.labels_, return_counts=True)\n",
    "\tfreqs = dict(zip(unique, counts))\n",
    "\n",
    "\tpercentage = freqs[-1] / len(scaled_X)\n",
    "\t\n",
    "\tmodels.append({'epsilon': eps, 'outliers': percentage})\n",
    "\n",
    "models_df = pd.DataFrame(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier points vs. eps value lineplot\n",
    "sns.lineplot(data=models_df, x='epsilon', y='outliers');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN with chosen epsilon value\n",
    "model = DBSCAN(eps=2, min_samples=min_samples)\n",
    "model.fit(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster to dataframe\n",
    "original_df['Cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the different clusterings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
